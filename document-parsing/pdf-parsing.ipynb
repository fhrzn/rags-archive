{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of document examples are obtained here: [https://www.princexml.com/samples/](https://www.princexml.com/samples/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = {\n",
    "    \"invoice1\": \"assets/invoice.pdf\",\n",
    "    \"invoice2\": \"assets/invoice2.pdf\",\n",
    "    \"brochure\": \"assets/brochure.pdf\",\n",
    "    \"newsletter\": \"assets/newsletter.pdf\",\n",
    "    \"textbook\": \"assets/textbook.pdf\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing PDF into Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyMuPDF4LLM\n",
    "\n",
    "Documentation: [https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/index.html](https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# InInvvoicoicee\n",
      "\n",
      "Customer Name\n",
      "Street\n",
      "Postcode City\n",
      "Country\n",
      "\n",
      "Invoice date: Nov 26, 2016\n",
      "\n",
      "Invoice number: 161126\n",
      "\n",
      "Payment due: 30 days after invoice date\n",
      "\n",
      "\n",
      "YesLogic Pty. Ltd.\n",
      "7 / 39 Bouverie St\n",
      "Carlton VIC 3053\n",
      "Australia\n",
      "\n",
      "www.yeslogic.com\n",
      "ABN 32 101 193 560\n",
      "\n",
      "\n",
      "**Description** **From** **Until** **Amount**\n",
      "\n",
      "Prince Upgrades & Support Nov 26, 2016 Nov 26, 2017 USD $950.00\n",
      "\n",
      "Total USD $950.00\n",
      "\n",
      "Please transfer amount to:\n",
      "\n",
      "Bank account name: [Yes Logic Pty Ltd](http://www.princexml.com)\n",
      "\n",
      "Name of Bank: [Commonwealth Bank of Australia (CBA)](https://www.commbank.com.au/)\n",
      "\n",
      "Bank State Branch (BSB): 063010\n",
      "\n",
      "Bank State Branch (BSB): 063010\n",
      "\n",
      "Bank State Branch (BSB): 063019\n",
      "\n",
      "Bank account number: 13201652\n",
      "\n",
      "Bank SWIFT code: CTBAAU2S\n",
      "\n",
      "Bank address: 231 Swanston St, Melbourne, VIC 3000, Australia\n",
      "\n",
      "The BSB number identifies a branch of a financial institution in Australia. When transferring money to Australia, the\n",
      "BSB number is used together with the bank account number and the SWIFT code. Australian banks do not use IBAN\n",
      "numbers.\n",
      "\n",
      "[www.yeslogic.com](http://www.yeslogic.com)\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoice1 = pymupdf4llm.to_markdown(filepath[\"invoice1\"], show_progress=False)\n",
    "print(invoice1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Denny Gunawan\n",
      "\n",
      "#### 221 Queen St Melbourne VIC 3000\n",
      "\n",
      "\n",
      "### 123 Somewhere St, Melbourne VIC 3000\n",
      "# $39.60\n",
      "\n",
      "### (03) 1234 5678\n",
      "\n",
      "#### Invoice Number: #20130304\n",
      "\n",
      "### Organic Items Price/kg Quantity(kg) Subtotal\n",
      "\n",
      "#### Apple $5.00 1 $5.00\n",
      "\n",
      " Orange $1.99 2 $3.98\n",
      "\n",
      " Watermelon $1.69 3 $5.07\n",
      "\n",
      " Mango $9.56 2 $19.12\n",
      "\n",
      " Peach $2.99 1 $2.99\n",
      "\n",
      "\n",
      "### Subtotal $36.00\n",
      "\n",
      " GST (10%) $3.60\n",
      "\n",
      "\n",
      "\n",
      "- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam sodales **Total** **$39.60**\n",
      "dapibus fermentum. Nunc adipiscing, magna sed scelerisque cursus, erat\n",
      "lectus dapibus urna, sed facilisis leo dui et ipsum.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "invoice2 = pymupdf4llm.to_markdown(filepath[\"invoice2\"], show_progress=False)\n",
    "print(invoice2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 221 Queen Street, Melbourne VIC 3000\n",
      "\n",
      "apartment **2** **1** **1**\n",
      "\n",
      "## WARM BEAUTIFUL APARTMENT\n",
      "\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc turpis ante, fermentum ac\n",
      "cursus vitae, tempus vel ipsum. Proin justo lorem, mattis sed pharetra ac, vehicula\n",
      "tincidunt sem. Mauris est leo, scelerisque at gravida vel, interdum quis felis. Nullam\n",
      "nibh magna, consectetur eu aliquet ac, sollicitudin eget nisl. Praesent nibh tellus,\n",
      "elementum ut scelerisque a, pretium eget libero. Morbi sodales tincidunt turpis et\n",
      "sagittis. Aenean sed dictum nisi.\n",
      "\n",
      "Phasellus quis metus lectus. Donec varius pellentesque purus, et ornare enim\n",
      "pellentesque sed. Morbi quam augue, pulvinar sit amet mollis in, euismod porta orci.\n",
      "Quisque rutrum egestas enim ac viverra. Mauris blandit tristique feugiat. Ut molestie\n",
      "molestie turpis sed elementum. Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n",
      "Maecenas in nunc odio. Nunc sagittis, mi ut ornare pellentesque, orci augue dapibus\n",
      "est, vel rutrum elit orci vel augue.\n",
      "\n",
      "\n",
      "# $1,000,000\n",
      "\n",
      "## Prince Cascading\n",
      "\n",
      "1234-5678\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brochure = pymupdf4llm.to_markdown(filepath[\"brochure\"], show_progress=False)\n",
    "print(brochure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# DrylabNews\n",
      "\n",
      "#### for investors & friends · May 2017\n",
      "\n",
      "\n",
      "Welcome to our first newsletter of 2017! It's\n",
      "been a while since the last one, and a lot has\n",
      "happened. We promise to keep them coming\n",
      "every two months hereafter, and permit\n",
      "ourselves to make this one rather long. The\n",
      "big news is the beginnings of our launch in\n",
      "the American market, but there are also\n",
      "interesting updates on sales, development,\n",
      "mentors and (of course) the investment\n",
      "round that closed in January.\n",
      "\n",
      "**New capital: The investment round was**\n",
      "successful. We raised 2.13 MNOK to match\n",
      "\n",
      "\n",
      "the 2.05 MNOK loan from Innovation\n",
      "Norway. Including the development\n",
      "agreement with Filmlance International, the\n",
      "total new capital is 5 MNOK, partly tied to\n",
      "the successful completion of milestones. All\n",
      "formalities associated with this process are\n",
      "now finalized.\n",
      "\n",
      "**New owners: We would especially like to**\n",
      "warmly welcome our new owners to the\n",
      "Drylab family: Unni Jacobsen, Torstein Jahr,\n",
      "Suzanne Bolstad, Eivind Bergene, Turid Brun,\n",
      "Vigdis Trondsen, Lea Blindheim, Kristine\n",
      "\n",
      "\n",
      "## 34\n",
      "\n",
      "### meetingsmeetings\n",
      " NY · SFNY · SF\n",
      " LA · LLA · LVV\n",
      "\n",
      "\n",
      "Academy of Motion Picture Arts and Sciences · Alesha & Jamie Metzger · Amazon\n",
      "AWS · Apple · Caitlin Burns, PGA · Carlos Melcer · Chimney L.A. · Dado Valentic ·\n",
      "Dave Stump · DIT WIT · ERA NYC · Facebook · Fancy Film · FilmLight · Geo Labelle ·\n",
      "Google · IBM · Innovation Norway (NYC) · Innovation Norway (SF) · International\n",
      "Cinematographers Guild · NBC · Local 871 · Netflix · Pomfort · Radiant Images ·\n",
      "Screening Room · Signiant · Moods of Norway · Tapad · Team Downey\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "Holmsen, Torstein Hansen, and Jostein\n",
      "Aanensen. We look forward to working with\n",
      "you!\n",
      "\n",
      "**Sales: Return customer rate is now 80%,**\n",
      "proving value and willingness to pay. Film\n",
      "Factory Montreal is our first customer in\n",
      "Canada. Lumiere Numeriques have started\n",
      "using us in France. We also have new\n",
      "customers in Norway, and high-profile users\n",
      "such as Gareth Unwin, producer of Oscar[winning The King's Speech. Revenue for the](http://www.imdb.com/title/tt1504320/)\n",
      "first four months is 200 kNOK, compared to\n",
      "339 kNOK for all of 2016. We are working\n",
      "on a partnership to safeguard sales in\n",
      "Norway while beginning to focus more on\n",
      "the US.\n",
      "\n",
      "**New team members: We've extended our**\n",
      "organization with two permanent developers\n",
      "based in Łódź, the film capital of Poland. Two\n",
      "highly skilled interns from the University of\n",
      "Oslo's Entrepreneurship Program, will be\n",
      "working on market research until mid-June\n",
      "(starting in March), preparing for the US\n",
      "launch. Also, two computer science students\n",
      "are working as part-time interns during\n",
      "spring, on machine learning and analysis\n",
      "research, as well as innovative architectures\n",
      "based on the Swift language. We hope our\n",
      "interns will consider sticking around!\n",
      "\n",
      "**New mentor: We are honored to have**\n",
      "Caitlin Burns joining us as a mentor. She's an\n",
      "\n",
      "\n",
      "accomplished producer based in New York,\n",
      "an active member of the Producers Guild of\n",
      "America, and the collaboration has already\n",
      "yielded good results, including valuable\n",
      "contacts for our visit in Los Angeles. Oscar[winning VFX supervisor Dave Stump joined](http://www.imdb.com/name/nm0003432/)\n",
      "us earlier.\n",
      "\n",
      "**New York, St. Louis, San Francisco and**\n",
      "**Los Angeles: Pontus and Audun did a tour**\n",
      "of the US in February and March, meeting\n",
      "users, partners and potential customers. The\n",
      "trip was very successful, with several high\n",
      "points, including meetings with Netflix, the\n",
      "Academy of Motion Picture Arts and\n",
      "Sciences, the International\n",
      "Cinematographers Guild, Local 871 (the\n",
      "script supervisors' union), one of the world's\n",
      "leading DITs, and Apple. See the separate\n",
      "attachment for a more detailed summary.\n",
      "\n",
      "**NAB: Andreas and Audun travelled to the**\n",
      "National Association of Broadcasters\n",
      "convention (NAB) in Las Vegas for three\n",
      "hectic days in April. NAB gathers 100,000\n",
      "participants from film and TV. It's a very\n",
      "efficient way of meeting people in the\n",
      "business, and getting an updated picture of\n",
      "the business landscape. The most exciting\n",
      "meeting was with PIX System, one of our\n",
      "most important competitors. It was\n",
      "interesting to note that they regarded the\n",
      "indie market as bigger than their own.\n",
      "\n",
      "Andreas was able to secure us an\n",
      "\n",
      "invitation to the DIT-WIT party, with some of\n",
      "the world's leading DITs in attendance. It was\n",
      "a great place for informal feedback on Drylab\n",
      "Viewer. The pattern was the same as for\n",
      "other users: Initial polite interest turns to\n",
      "real enthusiasm the moment someone is able\n",
      "to personally try Drylab Viewer! We also\n",
      "met with Pomfort and Apple about our ongoing collaborations; ARRI and Teradek/\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "Paralinx about camera integration; Amazon,\n",
      "Google and IBM about cloud computing.\n",
      "\n",
      "**WWDC and Silicon Valley: We were very**\n",
      "pleasantly surprised to be invited by Apple to\n",
      "their World Wide Developers Conference in\n",
      "San Jose in June, despite not having applied.\n",
      "It's a valuable chance to learn and make new\n",
      "connections. We’re also setting aside time to\n",
      "meet other potential partners.\n",
      "\n",
      "**Cine Gear: We have decided not to attend**\n",
      "the Cine Gear expo in L.A. this year, since\n",
      "feedback from many users about the show\n",
      "were mixed, and our planned beta version of\n",
      "3.0 is slightly delayed.\n",
      "\n",
      "**Development and launch: Development**\n",
      "is around one month behind our original\n",
      "schedule. We expect the delay to decrease,\n",
      "with new developers on board.\n",
      "\n",
      "\n",
      "The launch of Drylab 3.0 will take place at\n",
      "\n",
      "the International Broadcasters Convention\n",
      "in Amsterdam in September, and we are\n",
      "working hard to get solid feedback from pilot\n",
      "users before then.\n",
      "\n",
      "**Annual General Meeting: Drylab's AGM**\n",
      "will be held on June 16th at 15:00. An\n",
      "invitation will be distributed to all owners\n",
      "well in advance. We hope to see you there!\n",
      "\n",
      "**As you can see it has been a hectic**\n",
      "**spring that has given us a lot of**\n",
      "**confirmation about our product. We**\n",
      "**are now working eagerly and hard**\n",
      "**towards the US launch with Drylab**\n",
      "**3.0, while keeping momentum in**\n",
      "**Europe with our existing system.**\n",
      "\n",
      "\n",
      "\n",
      "[[Drylab has kindly allowed this newsletter to be redone in HTML/CSS and converted to PDF](http://www.drylab.no/)\n",
      "[with Prince. Navngen helped anonymize names in the process.]](http://www.princexml.com/)\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newsletter = pymupdf4llm.to_markdown(filepath[\"newsletter\"], show_progress=False)\n",
    "print(newsletter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Anatomy of the Somatosensory System\n",
      "\n",
      "FROM WIKIBOOKS[1]\n",
      "\n",
      "\n",
      "Our somatosensory system consists of sensors in the skin\n",
      "and sensors in our muscles, tendons, and joints. The receptors in the skin, the so called cutaneous receptors, tell\n",
      "us about temperature (thermoreceptors), pressure and surface texture (mechano receptors), and pain (nociceptors).\n",
      "The receptors in muscles and joints provide information\n",
      "about muscle length, muscle tension, and joint angles.\n",
      "\n",
      "## Cutaneous receptors\n",
      "\n",
      "Sensory information from Meissner corpuscles and rapidly\n",
      "adapting afferents leads to adjustment of grip force when\n",
      "objects are lifted. These afferents respond with a brief\n",
      "burst of action potentials when objects move a small distance during the early stages of lifting. In response to\n",
      "\n",
      "Hairy skin Glabrous skin\n",
      "\n",
      "Papillary Ridges\n",
      "\n",
      "Septa Epidermis\n",
      "\n",
      "\n",
      "_Figure 1:_ _Receptors in the hu-_\n",
      "\n",
      "_man skin: Mechanoreceptors can_\n",
      "_be free receptors or encapsulated._\n",
      "_Examples for free receptors are_\n",
      "_the hair receptors at the roots of_\n",
      "_hairs. Encapsulated receptors are_\n",
      "_the Pacinian corpuscles and the_\n",
      "_receptors in the glabrous (hair-_\n",
      "_less) skin: Meissner corpuscles,_\n",
      "_Ruffini corpuscles and Merkel’s_\n",
      "_disks._\n",
      "\n",
      "\n",
      "This is a sample document to\n",
      "showcase page-based formatting. It\n",
      "[contains a chapter from a Wikibook](http://en.wikibooks.org/)\n",
      "[called Sensory Systems. None of the](http://en.wikibooks.org/wiki/Sensory_Systems)\n",
      "content has been changed in this\n",
      "article, but some content has been\n",
      "removed.\n",
      "\n",
      "\n",
      "Meissne r’s\n",
      "Sebaceous corpuscle\n",
      "\n",
      "\n",
      "Dermis\n",
      "\n",
      "\n",
      "1 The following description is based on lecture notes from Laszlo Zaborszky, from Rutgers University.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "From Wikibooks\n",
      "\n",
      "_Figure 2:_ _Mammalian muscle_\n",
      "\n",
      "_spindle showing typical position_\n",
      "_in a muscle (left), neuronal con-_\n",
      "_nections in spinal cord (middle)_\n",
      "_and expanded schematic (right)._\n",
      "_The spindle is a stretch receptor_\n",
      "_with its own motor supply con-_\n",
      "_sisting of several intrafusal mus-_\n",
      "_cle fibres. The sensory endings of_\n",
      "_a primary (group Ia) afferent and_\n",
      "_a secondary (group II) afferent_\n",
      "_coil around the non-contractile_\n",
      "_central portions of the intrafusal_\n",
      "_fibres._\n",
      "\n",
      "\n",
      "rapidly adapting afferent activity, muscle force increases\n",
      "reflexively until the gripped object no longer moves. Such\n",
      "a rapid response to a tactile stimulus is a clear indication\n",
      "of the role played by somatosensory neurons in motor activity.\n",
      "\n",
      "The slowly adapting Merkel’s receptors are responsible\n",
      "\n",
      "for form and texture perception. As would be expected for\n",
      "receptors mediating form perception, Merkel’s receptors\n",
      "are present at high density in the digits and around the\n",
      "mouth (50/mm² of skin surface), at lower density in other glabrous surfaces, and at very low density in hairy skin.\n",
      "This innervations density shrinks progressively with the\n",
      "passage of time so that by the age of 50, the density in human digits is reduced to 10/mm². Unlike rapidly adapting\n",
      "axons, slowly adapting fibers respond not only to the initial indentation of skin, but also to sustained indentation\n",
      "up to several seconds in duration.\n",
      "\n",
      "Activation of the rapidly adapting Pacinian corpuscles\n",
      "\n",
      "gives a feeling of vibration, while the slowly adapting\n",
      "_Ruffini corpuscles respond to the lataral movement or_\n",
      "stretching of skin.\n",
      "\n",
      "## Nociceptors\n",
      "\n",
      "Nociceptors have free nerve endings. Functionally, skin\n",
      "nociceptors are either high-threshold mechanoreceptors\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "Anatomy of the Somatosensory System\n",
      "\n",
      "|Col1|Rapidly adapting|Slowly adapting|\n",
      "|---|---|---|\n",
      "|Surface receptor / small receptive field|Hair receptor, Meissner’s corpuscle: De- tect an insect or a very fine vibration. Used for recognizing texture.|Merkel’s receptor: Used for spa- tial details, e.g. a round surface edge or “an X” in brail.|\n",
      "|Deep receptor / large receptive field|Pacinian corpuscle: “A diffuse vibra- tion” e.g. tapping with a pencil.|Ruffini’s corpuscle: “A skin stretch”. Used for joint position in fingers.|\n",
      "\n",
      "\n",
      "_Table 1_\n",
      "\n",
      "or polymodal receptors. Polymodal receptors respond not\n",
      "only to intense mechanical stimuli, but also to heat and\n",
      "to noxious chemicals. These receptors respond to minute\n",
      "punctures of the epithelium, with a response magnitude\n",
      "that depends on the degree of tissue deformation. They also respond to temperatures in the range of 40–60°C, and\n",
      "change their response rates as a linear function of warming (in contrast with the saturating responses displayed by\n",
      "non-noxious thermoreceptors at high temperatures).\n",
      "\n",
      "Pain signals can be separated into individual compo\n",
      "nents, corresponding to different types of nerve fibers\n",
      "used for transmitting these signals. The rapidly transmitted signal, which often has high spatial resolution, is\n",
      "called first pain or cutaneous pricking pain. It is well localized and easily tolerated. The much slower, highly affective component is called second pain or burning pain; it is\n",
      "poorly localized and poorly tolerated. The third or deep\n",
      "_pain, arising from viscera, musculature and joints, is also_\n",
      "poorly localized, can be chronic and is often associated\n",
      "with referred pain.\n",
      "\n",
      "## Muscle Spindles\n",
      "\n",
      "Scattered throughout virtually every striated muscle in the\n",
      "body are long, thin, stretch receptors called muscle spindles. They are quite simple in principle, consisting of a few\n",
      "small muscle fibers with a capsule surrounding the middle\n",
      "third of the fibers. These fibers are called intrafusal fibers,\n",
      "in contrast to the ordinary extrafusal fibers. The ends of the\n",
      "intrafusal fibers are attached to extrafusal fibers, so whenever the muscle is stretched, the intrafusal fibers are also\n",
      "\n",
      "\n",
      "Notice how figure captions and\n",
      "sidenotes are shown in the outside\n",
      "margin (on the left or right, depending\n",
      "on whether the page is left or right).\n",
      "Also, figures are floated to the top/\n",
      "bottom of the page. Wide content, like\n",
      "the table and Figure 3, intrude into the\n",
      "outside margins.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "From Wikibooks\n",
      "\n",
      "Force\n",
      "control\n",
      "signal\n",
      "\n",
      "Driving\n",
      "signal\n",
      "\n",
      "Length (secondary muscle-spindel afferents)\n",
      "\n",
      "Length error (primary muscle-spindel afferents)\n",
      "\n",
      "Velocity (primary muscle-spindel afferents)\n",
      "\n",
      "Length\n",
      "control\n",
      "signal\n",
      "\n",
      "\n",
      "Force (Golgi tendon organ)\n",
      "\n",
      "Force feedback\n",
      "\n",
      "\n",
      "Muscle\n",
      "length\n",
      "\n",
      "|Inter- neurons|Col2|Col3|\n",
      "|---|---|---|\n",
      "||neurons||\n",
      "||||\n",
      "\n",
      "\n",
      "_Figure 3: Feedback loops for proprioceptive signals for the perception and control of limb move-_\n",
      "_ments. Arrows indicate excitatory connections; filled circles inhibitory connections._\n",
      "\n",
      "stretched. The central region of each intrafusal fiber has\n",
      "few myofilaments and is non-contractile, but it does have\n",
      "one or more sensory endings applied to it. When the muscle is stretched, the central part of the intrafusal fiber is\n",
      "stretched and each sensory ending fires impulses.\n",
      "\n",
      "\n",
      "For more examples of how to use\n",
      "\n",
      "HTML and CSS for paper-based\n",
      "\n",
      "[publishing, see css4.pub.](http://css4.pub)\n",
      "\n",
      "\n",
      "Muscle spindles also receive a motor innervation. The\n",
      "\n",
      "large motor neurons that supply extrafusal muscle fibers\n",
      "are called alpha motor neurons, while the smaller ones supplying the contractile portions of intrafusal fibers are\n",
      "called gamma neurons. Gamma motor neurons can regulate the sensitivity of the muscle spindle so that this sensitivity can be maintained at any given muscle length.\n",
      "\n",
      "## Joint receptors\n",
      "\n",
      "The joint receptors are low-threshold mechanoreceptors\n",
      "and have been divided into four groups. They signal different characteristics of joint function (position, movements,\n",
      "direction and speed of movements). The free receptors or\n",
      "type 4 joint receptors are nociceptors.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textbook = pymupdf4llm.to_markdown(filepath[\"textbook\"], show_progress=False)\n",
    "print(textbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langchain PyPDFLoader\n",
    "\n",
    "Documentation: [https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YesLogic Pty. Ltd.\n",
      "7 / 39 Bouverie St\n",
      "Carlton VIC 3053\n",
      "Australia\n",
      "www.yeslogic.com\n",
      "ABN 32 101 193 560InInvvoicoicee\n",
      "Customer Name\n",
      "Street\n",
      "Postcode City\n",
      "Country\n",
      "Invoice date: Nov 26, 2016\n",
      "Invoice number: 161126\n",
      "Payment due: 30 days after invoice date\n",
      "Description From Until Amount\n",
      "Prince Upgrades & Support Nov 26, 2016 Nov 26, 2017 USD $950.00\n",
      "Total USD $950.00\n",
      "Please transfer amount to:\n",
      "Bank account name: Yes Logic Pty Ltd\n",
      "Name of Bank: Commonwealth Bank of Australia (CBA)\n",
      "Bank State Branch (BSB): 063010\n",
      "Bank State Branch (BSB): 063010\n",
      "Bank State Branch (BSB): 063019\n",
      "Bank account number: 13201652\n",
      "Bank SWIFT code: CTBAAU2S\n",
      "Bank address: 231 Swanston St, Melbourne, VIC 3000, Australia\n",
      "The BSB number identifies a branch of a financial institution in Australia. When transferring money to Australia, the\n",
      "BSB number is used together with the bank account number and the SWIFT code. Australian banks do not use IBAN\n",
      "numbers.\n",
      "www.yeslogic.com\n"
     ]
    }
   ],
   "source": [
    "lc_invoice1 = \"\\n==================\\n\".join(doc.page_content for doc in list(PyPDFLoader(filepath[\"invoice1\"]).lazy_load()))\n",
    "print(lc_invoice1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denny Gunawan\n",
      "221 Queen St\n",
      "Melbourne VIC 3000\n",
      "$39.60 123 Somewhere St, Melbourne VIC 3000\n",
      "(03) 1234 5678\n",
      "Invoice Number: #20130304\n",
      "Organic Items Price/kg Quantity(kg) Subtotal\n",
      "Apple $5.00 1 $5.00\n",
      "Orange $1.99 2 $3.98\n",
      "Watermelon $1.69 3 $5.07\n",
      "Mango $9.56 2 $19.12\n",
      "Peach $2.99 1 $2.99\n",
      "Subtotal $36.00\n",
      "GST (10%) $3.60\n",
      "Total $39.60 * Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam sodales\n",
      "dapibus fermentum. Nunc adipiscing, magna sed scelerisque cursus, erat\n",
      "lectus dapibus urna, sed facilisis leo dui et ipsum.\n"
     ]
    }
   ],
   "source": [
    "lc_invoice2 = \"\\n==================\\n\".join(doc.page_content for doc in list(PyPDFLoader(filepath[\"invoice2\"]).lazy_load()))\n",
    "print(lc_invoice2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$1,000,000\n",
      "221 Queen Street, Melbourne VIC 3000\n",
      "apartment\n",
      "2\n",
      "1\n",
      "1\n",
      "Prince Cascading\n",
      "1234-5678\n",
      "Additional Information\n",
      "Land Size\n",
      "House Size\n",
      "Map Ref.\n",
      "Energy Rating\n",
      "Council Rates\n",
      "Water Rates\n",
      "1171m2\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "WARM BEAUTIFUL APARTMENT\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc turpis ante, fermentum ac\n",
      "cursus vitae, tempus vel ipsum. Proin justo lorem, mattis sed pharetra ac, vehicula\n",
      "tincidunt sem. Mauris est leo, scelerisque at gravida vel, interdum quis felis. Nullam\n",
      "nibh magna, consectetur eu aliquet ac, sollicitudin eget nisl. Praesent nibh tellus,\n",
      "elementum ut scelerisque a, pretium eget libero. Morbi sodales tincidunt turpis et\n",
      "sagittis. Aenean sed dictum nisi.\n",
      "Phasellus quis metus lectus. Donec varius pellentesque purus, et ornare enim\n",
      "pellentesque sed. Morbi quam augue, pulvinar sit amet mollis in, euismod porta orci.\n",
      "Quisque rutrum egestas enim ac viverra. Mauris blandit tristique feugiat. Ut molestie\n",
      "molestie turpis sed elementum. Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n",
      "Maecenas in nunc odio. Nunc sagittis, mi ut ornare pellentesque, orci augue dapibus\n",
      "est, vel rutrum elit orci vel augue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lc_brochure = \"\\n==================\\n\".join(doc.page_content for doc in list(PyMuPDFLoader(filepath[\"brochure\"]).lazy_load()))\n",
    "print(lc_brochure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drylab Newsfor in vestors & friends · Ma y 2017\n",
      "Welcome to our first newsletter of 2017! It's\n",
      "been a while since the last one, and a lot has\n",
      "happened. W e promise to k eep them coming\n",
      "every two months hereafter , and permit\n",
      "ourselv es to mak e this one r ather long. The\n",
      "big news is the beginnings of our launch in\n",
      "the American mark et, but there are also\n",
      "interesting updates on sales, de velopment,\n",
      "mentors and ( of course ) the in vestment\n",
      "round that closed in January .\n",
      "New c apital: The in vestment round was\n",
      "successful. W e raised 2.13 MNOK to matchthe 2.05 MNOK loan from Inno vation\n",
      "Norwa y. Including the de velopment\n",
      "agreement with Filmlance International, the\n",
      "total new capital is 5 MNOK, partly tied to\n",
      "the successful completion of milestones. All\n",
      "formalities associated with this process are\n",
      "now finalized.\n",
      "New o wners: We would especially lik e to\n",
      "warmly welcome our new owners to the\n",
      "Drylab family: Unni Jacobsen, T orstein Jahr ,\n",
      "Suzanne Bolstad, Eivind Bergene, T urid Brun,\n",
      "Vigdis T rondsen, L ea Blindheim, Kristine\n",
      "34meetingsmeetings\n",
      "NY · SFNY · SF\n",
      "LA · LLA · L VVAcadem yofMotion Picture Arts and Sciences ·Alesha &Jamie Metzger ·Amazon\n",
      "AWS ·Apple ·Caitlin Burns, PGA ·Carlos Melcer ·Chimne yL.A.·Dado Valentic ·\n",
      "DaveStump ·DIT WIT ·ERA NYC·Facebook ·Fancy Film ·FilmLight ·Geo Labelle ·\n",
      "Google ·IBM ·Inno vation Norwa y(NY C)·Inno vation Norwa y(SF) ·International\n",
      "Cinematogr aphers Guild ·NBC ·Local 871 ·Netflix ·Pomfort ·Radiant Images ·\n",
      "Screening Room · Signiant · Moods of Norwa y· Tapad · Team Downe y\n",
      "==================\n",
      "Holmsen, T orstein Hansen, and Jostein\n",
      "Aanensen. W e look forward to working with\n",
      "you!\n",
      "Sales: Return customer r ate is now 80%,\n",
      "pro ving value and willingness to pa y. Film\n",
      "Factory Montreal is our first customer in\n",
      "Canada. Lumiere Numeriques ha ve started\n",
      "using us in F rance. W e also ha ve new\n",
      "customers in Norwa y, and high-profile users\n",
      "such as Gareth Un win, producer of Oscar-\n",
      "winning The King's Speech . Re venue for the\n",
      "first four months is 200 kNOK, compared to\n",
      "339 kNOK for all of 2016. W e are working\n",
      "on a partnership to safeguard sales in\n",
      "Norwa y while beginning to focus more on\n",
      "the US.\n",
      "New team members: We've extended our\n",
      "organization with two permanent de velopers\n",
      "based in Łódź, the film capital of P oland. T wo\n",
      "highly skilled interns from the Univ ersity of\n",
      "Oslo 's Entrepreneurship Progr am, will be\n",
      "working on mark et research until mid-June\n",
      "(starting in March), preparing for the US\n",
      "launch. Also, two computer science students\n",
      "are working as part-time interns during\n",
      "spring, on machine learning and analysis\n",
      "research, as well as inno vativ e architectures\n",
      "based on the Swift language. W e hope our\n",
      "interns will consider sticking around!\n",
      "New ment or:We are honored to ha ve\n",
      "Caitlin Burns joining us as a mentor . She 's anaccomplished producer based in New Y ork,\n",
      "an activ e member of the Producers Guild of\n",
      "America, and the collabor ation has already\n",
      "yielded good results, including valuable\n",
      "contacts for our visit in L os Angeles. Oscar-\n",
      "winning VFX supervisor Dave Stump joined\n",
      "us earlier .\n",
      "New Y ork, St. Louis, San Fr ancisc o and\n",
      "Los Angeles: Pontus and Audun did a tour\n",
      "of the US in F ebruary and March, meeting\n",
      "users, partners and potential customers. The\n",
      "trip was v ery successful, with se veral high\n",
      "points, including meetings with Netflix, the\n",
      "Academ y of Motion Picture Arts and\n",
      "Sciences, the International\n",
      "Cinematogr aphers Guild, L ocal 871 (the\n",
      "script supervisors' union), one of the world's\n",
      "leading DIT s, and Apple. See the separ ate\n",
      "attachment for a more detailed summary .\n",
      "NAB: Andreas and Audun tr avelled to the\n",
      "National Association of Broadcasters\n",
      "con vention (NAB) in Las V egas for three\n",
      "hectic da ys in April. NAB gathers 100,000\n",
      "participants from film and TV . It's a v ery\n",
      "efficient wa y of meeting people in the\n",
      "business, and getting an updated picture of\n",
      "the business landscape. The most e xciting\n",
      "meeting was with PIX System, one of our\n",
      "most important competitors. It was\n",
      "interesting to note that the y regarded the\n",
      "indie mark et as bigger than their own.\n",
      "Andreas was able to secure us an\n",
      "invitation to the DIT -WIT party , with some of\n",
      "the world's leading DIT s in attendance. It was\n",
      "a great place for informal feedback on Drylab\n",
      "Viewer . The pattern was the same as for\n",
      "other users: Initial polite interest turns to\n",
      "real enthusiasm the moment someone is able\n",
      "to personally try Drylab Viewer! W e also\n",
      "met with P omfort and Apple about our on-\n",
      "going collabor ations; ARRI and T eradek/\n",
      "==================\n",
      "Paralinx about camer a integr ation; Amazon,\n",
      "Google and IBM about cloud computing.\n",
      "WWD C and Silic on V alle y:We were v ery\n",
      "pleasantly surprised to be in vited b y Apple to\n",
      "their W orld Wide De velopers Conference in\n",
      "San Jose in June, despite not ha ving applied.\n",
      "It's a valuable chance to learn and mak e new\n",
      "connections. W e’re also setting aside time to\n",
      "meet other potential partners.\n",
      "Cine Gear: We ha ve decided not to attend\n",
      "the Cine Gear e xpo in L .A. this y ear, since\n",
      "feedback from man y users about the show\n",
      "were mix ed, and our planned beta v ersion of\n",
      "3.0 is slightly dela yed.\n",
      "Development and launch: Development\n",
      "is around one month behind our original\n",
      "schedule. W e expect the dela y to decrease,\n",
      "with new de velopers on board.The launch of Drylab 3.0 will tak e place at\n",
      "the International Broadcasters Con vention\n",
      "in Amsterdam in September , and we are\n",
      "working hard to get solid feedback from pilot\n",
      "users before then.\n",
      "Annual Gener al Meeting: Drylab 's A GM\n",
      "will be held on June 16th at 15:00. An\n",
      "invitation will be distributed to all owners\n",
      "well in advance. W e hope to see y ou there!\n",
      "As y ou c an see it has been a hectic\n",
      "spring that has giv en us a lot o f\n",
      "confirmation about our product. W e\n",
      "are no w w orking eagerly and hard\n",
      "towards the US launch with Dr ylab\n",
      "3.0, while k eeping momentum in\n",
      "Europe with our e xisting s ystem.\n",
      "[Drylab has kindly allowed this newsletter to be redone in HTML/CSS and con verted to PDF\n",
      "with Prince .Navngen helped anon ymize names in the process.]\n"
     ]
    }
   ],
   "source": [
    "lc_newsletter = \"\\n==================\\n\".join(doc.page_content for doc in list(PyPDFLoader(filepath[\"newsletter\"]).lazy_load()))\n",
    "print(lc_newsletter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample document to\n",
      "showcase page-based formatting. It\n",
      "contains a chapter from a Wikibook\n",
      "called Sensory Systems . None of the\n",
      "content has been changed in this\n",
      "article, but some content has been\n",
      "remo ved.Anat omy of the Somat osensor y System\n",
      "FROM WIKIBOOKS1\n",
      "Our somatosensor ysystem consists ofsensors intheskin\n",
      "and sensors inour muscles, tendons, and joints. The re-\n",
      "ceptors intheskin, thesocalled cutaneous receptors, tell\n",
      "usabout temperature (thermorec eptors ),pressure and sur-\n",
      "facetexture (mechano receptors ),and pain (nocic eptors ).\n",
      "The receptors inmuscles and joints provide information\n",
      "about muscle length, muscle tension, and joint angles.\n",
      "Cutaneous r ecept ors\n",
      "Sensor yinformation from Meissner corpuscles and rapidly\n",
      "adap ting afferents leads toadjustment ofgrip forcewhen\n",
      "objec tsare lifted. These afferents respond with abrief\n",
      "burst ofaction potentials when objec tsmoveasmall dis-\n",
      "tanc eduring the early stages oflifting. Inresponse to\n",
      "Figure 1:Receptors inthehu-\n",
      "man skin: Mechanorec eptors can\n",
      "befreereceptors orencapsulated.\n",
      "Examples forfree receptors are\n",
      "thehair receptors attheroots of\n",
      "hairs. Encapsulated receptors are\n",
      "thePacinian corpuscles and the\n",
      "receptors intheglabrous (hair-\n",
      "less) skin: Meissner corpuscles,\n",
      "Ruffini corpuscles and Merk el’s\n",
      "disks.Hairy skin Glabrous skin\n",
      "Epidermis\n",
      "Dermis\n",
      "Pacinian\n",
      "corpusclePapillary Ridges\n",
      "Septa\n",
      "Ruffini ’s\n",
      " corpuscle\n",
      "Hair receptorMeissne r’s\n",
      "corpuscleSebaceous\n",
      "glandFree nerve\n",
      "endingMerkel ’s\n",
      "receptor\n",
      "1The f ollowing descrip tion is based on lec ture no tes from Laszlo Zaborszky , from Rutgers Univ ersit y.\n",
      "1\n",
      "==================\n",
      "Figure 2: Mammalian muscle\n",
      "spindle showing typicalposition\n",
      "inamuscle (left), neuronal con-\n",
      "nections inspinal cord (middle )\n",
      "and expanded schematic (right).\n",
      "Thespindle isastretch receptor\n",
      "with itsown motor supply con-\n",
      "sisting ofseveral intrafusal mus-\n",
      "clefibres. Thesensory endings of\n",
      "aprimary (group Ia)afferent and\n",
      "asecondary (group II)afferent\n",
      "coilaround the non-c ontractile\n",
      "central portions oftheintrafusal\n",
      "fibres.\n",
      "rapidly adap ting afferent activit y,muscle forceincreases\n",
      "refle xivelyuntil thegripped objec tnolonger moves.Such\n",
      "arapid response toatactilestimulus isaclear indication\n",
      "oftherole play edbysomatosensor yneurons inmotorac-\n",
      "tivit y.\n",
      "The slowly adap ting Merk el’sreceptors areresponsible\n",
      "forform and texture perc eption. Aswould beexpec tedfor\n",
      "receptors mediating form perc eption, Merk el’sreceptors\n",
      "arepresent athigh densit yinthe digits and around the\n",
      "mouth (50/mm² ofskin surfac e),atlowerdensit yinoth-\n",
      "erglabrous surfac es,and atverylowdensit yinhairyskin.\n",
      "This inner vations densit yshrinks progressiv elywith the\n",
      "passage oftime sothat bytheageof50,thedensit yinhu-\n",
      "man digits isreduc edto10/mm². Unlik erapidly adap ting\n",
      "axons, slowly adap ting fibers respond notonly totheini-\n",
      "tialindentation ofskin, butalso tosustained indentation\n",
      "up to se veral sec onds in duration.\n",
      "Activation ofthe rapidly adap ting Pacinian corpuscles\n",
      "givesafeeling ofvibration, while the slowly adap ting\n",
      "Ruffini corpuscles respond tothe lataral movement or\n",
      "stretching of skin.\n",
      "Nocicept ors\n",
      "Nocic eptors havefree nerveendings. Func tionally ,skin\n",
      "nocic eptors areeither high-threshold mechanorec eptorsFrom Wikibooks\n",
      "2\n",
      "==================\n",
      "Rapidly adap ting Slowly adap ting\n",
      "Surfac ereceptor/\n",
      "small rec eptive\n",
      "fieldHair rec eptor ,Meissner’s c orpuscle : De-\n",
      "tect an insec t or a v ery fine vibration.\n",
      "Used f or rec ognizing te xture.Merk el’s rec eptor: Used f or spa-\n",
      "tial details, e.g. a round surfac e\n",
      "edge or “ an X” in brail.\n",
      "Deep rec eptor /\n",
      "large rec eptive\n",
      "fieldPacinian c orpuscle : “A diffuse vibra-\n",
      "tion” e.g. tapping with a pencil.Ruffini’s c orpuscle : “A skin\n",
      "stretch” . Used f or joint position\n",
      "in fingers.\n",
      "Table 1\n",
      "Notice how figure captions and\n",
      "sidenotes are shown in the outside\n",
      "margin ( on the left or right, depending\n",
      "on whether the page is left or right).\n",
      "Also, figures are floated to the top/\n",
      "bottom of the page. Wide content, lik e\n",
      "the table and Figure 3, intrude into the\n",
      "outside margins.orpolymodal receptors .Polymodal receptors respond not\n",
      "only tointense mechanical stimuli, but also toheat and\n",
      "tonoxious chemicals. These receptors respond tominute\n",
      "punc tures ofthe epithelium, with aresponse magnitude\n",
      "that depends onthedegree oftissue deformation. Theyal-\n",
      "sorespond totemperatures intherange of40–60°C, and\n",
      "change their response rates asalinear func tion ofwarm-\n",
      "ing(incontrast with thesaturating responses display edby\n",
      "non-no xious thermorec eptors at high temperatures).\n",
      "Painsignals can beseparated into individual compo-\n",
      "nents, corresponding todifferent types ofnervefibers\n",
      "used fortransmit ting these signals. The rapidly transmit-\n",
      "ted signal, which often has high spatial resolution, is\n",
      "called first pain orcutaneous pricking pain .Itiswelllocal-\n",
      "izedand easily tolerated. The much slower,highly affec-\n",
      "tivecomponent iscalled second pain orburning pain ;itis\n",
      "poorly localiz edand poorly tolerated. The third ordeep\n",
      "pain ,arising from viscera, musculature and joints, isalso\n",
      "poorly localiz ed,can bechronic and isoften associated\n",
      "with ref erred pain.\n",
      "Muscle Spindles\n",
      "Scat tered throughout virtually everystriated muscle inthe\n",
      "body arelong, thin, stretch receptors called muscle spin-\n",
      "dles. Theyarequite simple inprinciple, consisting ofafew\n",
      "small muscle fibers with acapsule surrounding themiddle\n",
      "third ofthefibers. These fibers arecalled intrafusal fibers ,\n",
      "incontrast totheordinar yextrafusal fibers .The ends ofthe\n",
      "intrafusal fibers areattached toextrafusal fibers, sowhen-\n",
      "everthemuscle isstretched, theintrafusal fibers arealsoAnatom y of the Somatosensory System\n",
      "3\n",
      "==================\n",
      "Force\n",
      "control\n",
      "signal\n",
      "Driving\n",
      "signal\n",
      "Length\n",
      "control\n",
      "signalLoadExternal\n",
      "forces\n",
      "Tendon\n",
      "organsMuscle forceMuscle\n",
      "lengthForce feedback\n",
      "Length &\n",
      "velocity\n",
      "feedbackForce (Golgi tendon organ)\n",
      "Spindles\n",
      "Gamma biasLength (secondary muscle-spindel afferents)\n",
      "Length error (primary muscle-spindel afferents)\n",
      "Velocity (primary muscle-spindel afferents)MuscleInter-\n",
      "neurons\n",
      "Figure 3:Feedback loops forproprioc eptiv esignals fortheperception and control oflimb move-\n",
      "ments. A rrows indic ate ex citatory c onnections; filled circles inhibitory c onnections.\n",
      "For more e xamples of how to use\n",
      "HTML and CSS for paper-based\n",
      "publishing, see css4.pub .stretched. The central region ofeach intrafusal fiber has\n",
      "fewmyofilaments and isnon-c ontrac tile, butitdoes have\n",
      "one ormore sensor yendings applied toit.When themus-\n",
      "cleisstretched, thecentral part oftheintrafusal fiber is\n",
      "stretched and each sensor y ending fires impulses.\n",
      "Muscle spindles also receiveamotorinner vation. The\n",
      "large motorneurons that supply extrafusal muscle fibers\n",
      "arecalled alpha motor neurons ,while thesmaller ones sup-\n",
      "plying the contrac tile portions ofintrafusal fibers are\n",
      "called gamma neurons .Gamma motorneurons can regu-\n",
      "late thesensitivit yofthemuscle spindle sothat this sensi-\n",
      "tivit y can be maintained at any giv en muscle length.\n",
      "Joint r ecept ors\n",
      "The joint receptors arelow-threshold mechanorec eptors\n",
      "and havebeen divided into fourgroups. Theysignal differ-\n",
      "entcharac teristics ofjoint func tion (position, movements,\n",
      "direc tion and speed ofmovements). The free receptors or\n",
      "type 4 joint rec eptors are nocic eptors.From Wikibooks\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "lc_textbook = \"\\n==================\\n\".join(doc.page_content for doc in list(PyPDFLoader(filepath[\"textbook\"]).lazy_load()))\n",
    "print(lc_textbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Website into Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Jina AI Reader\n",
    "Documentation: [https://jina.ai/reader/](https://jina.ai/reader/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://r.jina.ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://r.jina.ai/https://medium.com/data-folks-indonesia/exploring-visual-transformers-vit-with-huggingface-8cdda82920a0\n"
     ]
    }
   ],
   "source": [
    "site = \"https://medium.com/data-folks-indonesia/exploring-visual-transformers-vit-with-huggingface-8cdda82920a0\"\n",
    "\n",
    "url = os.path.join(BASE_URL, site)\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get(url)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Vision Transformers (ViT) with 🤗 Huggingface | Data Folks Indonesia\n",
      "\n",
      "URL Source: https://medium.com/data-folks-indonesia/exploring-visual-transformers-vit-with-huggingface-8cdda82920a0\n",
      "\n",
      "Published Time: 2022-10-14T11:00:46.983Z\n",
      "\n",
      "Markdown Content:\n",
      "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2021)\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[![Image 1: Affandy Fahrizain](https://miro.medium.com/v2/resize:fill:88:88/1*JCueIcAZjfbCE_ro4ZB8Og.jpeg)](https://medium.com/@fahrizain?source=post_page-----8cdda82920a0--------------------------------)[![Image 2: Data Folks Indonesia](https://miro.medium.com/v2/resize:fill:48:48/1*s8T4-0fscxMhh6V8adR4mQ.png)](https://medium.com/data-folks-indonesia?source=post_page-----8cdda82920a0--------------------------------)\n",
      "\n",
      "Lately, I was working on a course project where we asked to review one of the modern DL papers from top latest conferences and make an experimental test with our own dataset. So, here I am thrilled to share with you about my exploration!\n",
      "\n",
      "![Image 3](https://miro.medium.com/v2/resize:fit:700/0*et8V-t6bjFm1w6ds)\n",
      "\n",
      "Photo by [Alex Litvin](https://unsplash.com/@alexlitvin?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)\n",
      "\n",
      "Background\n",
      "----------\n",
      "\n",
      "As self-attention based model like Transformers has successfully become a _standard_ in NLP area, it triggers researchers to adapt attention-based models in Computer Vision too. There were different evidences, such as combine CNN with self-attention and completely replace Convolutions. While this selected paper belongs to the latter aproach.\n",
      "\n",
      "The application of attention mechanism in images requires each pixel attends to every other pixel, which indeed requires expensive computation. Hence, several techniques have been applied such as self-attention only in local neighborhoods \\[1\\], using local multihead dot product self-attention blocks to completely replace convolutions \\[2\\]\\[3\\]\\[4\\], postprocessing CNN outputs using self- attention \\[5\\]\\[6\\], etc. Although shown promising results, these techniques quite hard to be scaled and requires complex engineering to be implemented efficiently on hardware accelerators.\n",
      "\n",
      "On the other hand, Transformers model is based on MLP networks, it has more computational efficiency and scalability, making its possible to train big models with over 100B parameters.\n",
      "\n",
      "Methods\n",
      "-------\n",
      "\n",
      "![Image 4](https://miro.medium.com/v2/resize:fit:700/1*-HQPfbnebarylP543i58_Q.png)\n",
      "\n",
      "General architecture of ViT. Taken from the original paper (Dosovitskiy et al., 2021)\n",
      "\n",
      "The original Transformers model treat its input as sequences which very different approach with CNN, hence the inputted images need to be extracted into fixed-size patches and flattened. Similar to BERT \\[CLS\\] token, the so-called _classification token_ will be added into the beginning of the sequences, which will serve as image representation and later will be fed into classification head. Finally, to retain the positional information of the sequences, positional embedding will be added to each patch.\n",
      "\n",
      "The authors designed model following the original Transformers as close as possible. The proposed model then called as Vision Transfomers (ViT).\n",
      "\n",
      "Experiments\n",
      "-----------\n",
      "\n",
      "The authors released 3 variants of ViT; ViT-Base, ViT-Large, and ViT-Huge with different number of layers, hidden layers, MLP size, attention heads, and number of params. All of these are pretrained on large dataset such as ImageNet, ImageNet-21k, and JFT.\n",
      "\n",
      "In the original paper, the author compared ViT with ResNet based models like BiT. The result shows ViT outperform ResNet based models while taking less computational resources to pretrain.\n",
      "\n",
      "The following section will become technical part where we will use 🤗 Huggingface implementation of ViT to finetune our selected dataset.\n",
      "\n",
      "🤗 Huggingface in Action\n",
      "------------------------\n",
      "\n",
      "Now, let’s do interesting part. Here we will finetune ViT-Base using [Shoe vs Sandal vs Boot dataset](https://www.kaggle.com/datasets/hasibalmuzdadid/shoe-vs-sandal-vs-boot-dataset-15k-images) publicly available in Kaggle and examine its performance.\n",
      "\n",
      "First, lets load the dataset using 🤗 Datasets.\n",
      "\n",
      "from torch.utils.data import DataLoader  \n",
      "from datasets import load\\_datasetdatasets = load\\_dataset('imagefolder', data\\_dir='../input/shoe-vs-sandal-vs-boot-dataset-15k-images/Shoe vs Sandal vs Boot Dataset')datasets\\_split = datasets\\['train'\\].train\\_test\\_split(test\\_size=.2, seed=42)  \n",
      "datasets\\['train'\\] = datasets\\_split\\['train'\\]  \n",
      "datasets\\['validation'\\] = datasets\\_split\\['test'\\]\n",
      "\n",
      "Lets examine some of our dataset\n",
      "\n",
      "\\# plot samples  \n",
      "samples = datasets\\['train'\\].select(range(6))  \n",
      "pointer = 0  \n",
      "fig, ax = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(10,6))for i in range(2):  \n",
      "    for j in range(3):  \n",
      "        ax\\[i,j\\].imshow(samples\\[pointer\\]\\['image'\\])  \n",
      "        ax\\[i,j\\].set\\_title(f\"{labels\\[samples\\[pointer\\]\\['label'\\]\\]} ({samples\\[pointer\\]\\['label'\\]})\")  \n",
      "        ax\\[i,j\\].axis('off')  \n",
      "        pointer+=1plt.show()\n",
      "\n",
      "![Image 5](https://miro.medium.com/v2/resize:fit:571/1*c4RvCzuh84nsUw5kn28PqA.png)\n",
      "\n",
      "Few of our dataset looks like\n",
      "\n",
      "Next, as we already know, we need to transform our images into fixed-size patches and flatten it. We also need to add positional encoding and the _classification token._ Here we will use 🤗 Huggingface Feature Extractor module which do all mechanism for us!\n",
      "\n",
      "This Feature Extractor is just like Tokenizer in NLP. Let’s now import the pretrained ViT and use it as Feature Extractor, then we will examine the outputs of processed image. Here we will use pretrained ViT with `patch_size=16` and pretrained on ImageNet21K dataset with resolution 224x224.\n",
      "\n",
      "model\\_ckpt = 'google/vit-base-patch16-224-in21k'  \n",
      "device = torch.device('cuda' if torch.cuda.is\\_available() else 'cpu')  \n",
      "extractor = ViTFeatureExtractor.from\\_pretrained(model\\_ckpt)extractor(samples\\[0\\]\\['image'\\], return\\_tensors='pt')\n",
      "\n",
      "![Image 6](https://miro.medium.com/v2/resize:fit:429/1*zrS0kcR2kBmOtLpFCuHa2Q.png)\n",
      "\n",
      "Our extracted features looks like\n",
      "\n",
      "Note that our original image has white background, that’s why our extracted features having a lot of `1.` value. Don’t worry, its normal, everything will be work :)\n",
      "\n",
      "Let’s proceed to the next step. Now we want to implement this feature extractor to the whole of our dataset. Generally, we could use `.map()` function from 🤗 Huggingface, but in this case it would be slow and time consuming. Instead, we will use `.with_transform()` function which will do transformation on the fly!\n",
      "\n",
      "def batch\\_transform(examples):  \n",
      "    # take a list of PIL images and turn into pixel values  \n",
      "    inputs = extractor(\\[x for x in examples\\['image'\\]\\], return\\_tensors='pt')  \n",
      "    # add the labels in  \n",
      "    inputs\\['label'\\] = examples\\['label'\\]return inputs\n",
      "\n",
      "transformed\\_data = datasets.with\\_transform(batch\\_transform)\n",
      "\n",
      "OK, so far we’re good. Next, let’s define our data collator function and evaluation metrics.\n",
      "\n",
      "\\# data collator  \n",
      "def collate\\_fn(examples):  \n",
      "    return {  \n",
      "        'pixel\\_values': torch.stack(\\[x\\['pixel\\_values'\\] for x in examples\\]),  \n",
      "        'labels': torch.tensor(\\[x\\['label'\\] for x in examples\\])  \n",
      "    }\\# metrics  \n",
      "metric = load\\_metric('accuracy')  \n",
      "def compute\\_metrics(p):  \n",
      "    labels = p.label\\_ids  \n",
      "    preds = p.predictions.argmax(-1)  \n",
      "    acc = accuracy\\_score(labels, preds)  \n",
      "    f1 = f1\\_score(labels, preds, average='weighted')  \n",
      "    return {  \n",
      "        'accuracy': acc,  \n",
      "        'f1': f1  \n",
      "    }\n",
      "\n",
      "Now, let’s load the model. Remember that we have 3 labels in our data, and we attach it as our model parameters, so we will have ViT with classification head output of 3.\n",
      "\n",
      "model = ViTForImageClassification.from\\_pretrained(  \n",
      "    model\\_ckpt,  \n",
      "    num\\_labels=len(labels),  \n",
      "    id2label={str(i): c for i, c in enumerate(labels)},  \n",
      "    label2id={c: str(i) for i, c in enumerate(labels)}  \n",
      ")  \n",
      "model = model.to(device)\n",
      "\n",
      "Let’s have some fun before we finetune our model! (This step is optional, if you want to jump into fine-tuning step, you can skip this section).\n",
      "\n",
      "I am quite interested to see ViT performance in zero-shot scenario. In case you are unfamiliar with _zero-shot_ term, it just barely use pretrained model to predict our new images. Keep in mind that most of pretrained model are trained on large datasets, so in _zero-shot_ scenario we want to take benefit from those large dataset for our model to identify features in another image that might haven’t see it before and then make a prediction. Let’s just see how it works in the code!\n",
      "\n",
      "\\# get our transformed dataset  \n",
      "zero\\_loader = DataLoader(transformed\\_data\\['test'\\], batch\\_size=16)  \n",
      "zero\\_pred = \\[\\]\\# zero-shot prediction  \n",
      "for batch in tqdm(zero\\_loader):  \n",
      "    with torch.no\\_grad():  \n",
      "        logits = model(batch\\['pixel\\_values'\\].to(device)).logits  \n",
      "        pred = logits.argmax(-1).cpu().detach().tolist()  \n",
      "        zero\\_pred += \\[labels\\[i\\] for i in pred\\]zero\\_true = \\[labels\\[i\\] for i in datasets\\['test'\\]\\['label'\\]\\]\\# plot confusion matrix  \n",
      "cm = confusion\\_matrix(zero\\_true, zero\\_pred, labels=labels)  \n",
      "disp = ConfusionMatrixDisplay(cm, display\\_labels=labels)  \n",
      "disp.plot()  \n",
      "plt.show()\\# metrics  \n",
      "print(f'Acc: {accuracy\\_score(zero\\_true, zero\\_pred):.3f}')  \n",
      "print(f'F1: {f1\\_score(zero\\_true, zero\\_pred, average=\"weighted\"):.3f}')\n",
      "\n",
      "In short, we put our transformed data in DataLoader which going to be transformed on the fly. Then, for every batch, we pass our transformed data into our pretrained model. Next, we take the logits only from the model output. Remember that we have classification head with number of output 3. So, for each inferred image we will have 3 logits score. Among these 3 score, we will take the maximum one and return its index using `.argmax()`. Finally, we plot our confusion matrix and print the accuracy and F1 score.\n",
      "\n",
      "![Image 7](https://miro.medium.com/v2/resize:fit:337/1*o0KeIxC7nfv3-v43EBqPDA.png)\n",
      "\n",
      "ViT confusion matrix on zero-shot scenario\n",
      "\n",
      "Surprisingly, we got a unsatisfied metrics score with `Accuracy: 0.329` and `F1-Score: 0.307`. OK, next let’s fine-tune our model for 3 epochs and test the performance again. Here, I used Kaggle environment to train model.\n",
      "\n",
      "batch\\_size = 16  \n",
      "logging\\_steps = len(transformed\\_data\\['train'\\]) // batch\\_sizetraining\\_args = TrainingArguments(  \n",
      "    output\\_dir='./kaggle/working/',  \n",
      "    per\\_device\\_train\\_batch\\_size=batch\\_size,  \n",
      "    per\\_device\\_eval\\_batch\\_size=batch\\_size,  \n",
      "    evaluation\\_strategy='epoch',  \n",
      "    save\\_strategy='epoch',  \n",
      "    num\\_train\\_epochs=3,  \n",
      "    fp16=True if torch.cuda.is\\_available() else False,  \n",
      "    logging\\_steps=logging\\_steps,  \n",
      "    learning\\_rate=1e-5,  \n",
      "    save\\_total\\_limit=2,  \n",
      "    remove\\_unused\\_columns=False,  \n",
      "    push\\_to\\_hub=False,  \n",
      "    load\\_best\\_model\\_at\\_end=True)trainer = Trainer(  \n",
      "    model=model,  \n",
      "    args=training\\_args,  \n",
      "    data\\_collator=collate\\_fn,  \n",
      "    compute\\_metrics=compute\\_metrics,  \n",
      "    train\\_dataset=transformed\\_data\\['train'\\],  \n",
      "    eval\\_dataset=transformed\\_data\\['validation'\\],  \n",
      "    tokenizer=extractor)trainer.train()\n",
      "\n",
      "The code above was responsible to train our model. Note that we used 🤗 Huggingface Trainer instead of write our own training loop. Next, lets examine our Loss, Accuracy, and F1 Score for each epochs. You can also specify WandB or Tensorboard in Trainer parameter `report_to` for better logging interface. (Honestly, here I am using wandb for logging purpose. But for simplicity, I skipped the explanation of wandb part)\n",
      "\n",
      "![Image 8](https://miro.medium.com/v2/resize:fit:700/1*P_yuwU4yPELwlUV8Xb1EcA.png)\n",
      "\n",
      "Model performances on each epochs\n",
      "\n",
      "Impressive, isn’t it? Our ViT model already got very high performance since the first epoch, and changing quite steadily! Finally, let’s test again on the test data and later we plot our model prediction on few of our test data.\n",
      "\n",
      "\\# inference on test data  \n",
      "predictions = trainer.predict(transformed\\_data\\['test'\\])  \n",
      "predictions.metrics  \n",
      "\\# plot samples  \n",
      "samples = datasets\\['test'\\].select(range(6))  \n",
      "pointer = 0fig, ax = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(10,6))  \n",
      "for i in range(2):  \n",
      "    for j in range(3):  \n",
      "        ax\\[i,j\\].imshow(samples\\[pointer\\]\\['image'\\])  \n",
      "        ax\\[i,j\\].set\\_title(f\"A: {labels\\[samples\\[pointer\\]\\['label'\\]\\]}\\\\nP: {labels\\[predictions.label\\_ids\\[pointer\\]\\]}\")  \n",
      "        ax\\[i,j\\].axis('off')  \n",
      "        pointer+=1plt.show()\n",
      "\n",
      "Here is our prediction scores on test data. Our finetuned model now has a very good performances compared to the one in _zero-shot_ scenario. And among of 6 sampled test images, our model correctly predict all of them. Super! ✨\n",
      "\n",
      "{'test\\_loss': 0.04060511291027069,    \n",
      " 'test\\_accuracy': 0.994,    \n",
      " 'test\\_f1': 0.9939998484491527,    \n",
      " 'test\\_runtime': 30.7084,    \n",
      " 'test\\_samples\\_per\\_second': 97.693,    \n",
      " 'test\\_steps\\_per\\_second': 6.122}\n",
      "\n",
      "![Image 9](https://miro.medium.com/v2/resize:fit:571/1*FCx445gVXRtjQ69YXVECbQ.png)\n",
      "\n",
      "Conclusion\n",
      "----------\n",
      "\n",
      "Finally, we reached the end of the article. To recap, we did quick review of the original paper of Vision Transformers (ViT). We also perform _zero-shot_ and finetuning scenario to our pretrained model using publicly available Kaggle Shoe vs Sandals vs Boots dataset containing ~15K images. We examined that ViT performance on _zero-shot_ scenario wasn’t so good, while after finetuning the performance boost up since the first epoch and changing steadily.\n",
      "\n",
      "If you found this article is useful, please don’t forget to clap and follow me for more Data Science / Machine Learning contents. Also, if you found something wrong or interesting, please feel free to drop it in the comment or reach me out at Twitter or Linkedin.\n",
      "\n",
      "In case you are interested to read more, follow our medium [Data Folks Indonesia](https://medium.com/data-folks-indonesia) and don’t forget join us [Jakarata AI Research on Discord](https://discord.com/invite/6v28dq8dRE)!\n",
      "\n",
      "Full codes are available on my [Github repository](https://github.com/fhrzn/sml-tech/blob/main/Tasks/Course%20Project/vit-shoe-vs-sandals.ipynb), feel free to check it 🤗.\n",
      "\n",
      "_NB: If you are looking for deeper explanation especially if you want to reproduce the paper by yourself, you can check this_ [_amazing article by Aman Arora_](https://amaarora.github.io/2021/01/18/ViT.html)_._\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resp.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
